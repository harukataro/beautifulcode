---
template: post
slug: gpt-3
draft: false
socialImage: /media/image-2.jpg
title: 海外で話題になりまくっているAI GPT-3の説明を
date: 2020-07-21T02:16:12.190Z
description: GPT-3という自然言語を対象としたAIがすごいと噂。しらべてみることにした
category: AI
tags:
  - AI
socialImage: /media/downtown-gray.png
---

## 非常に興奮している

今週起こった出来事に驚きを隠せない。アメリカ発の AI がとんでもなくすごいのだ。

**たとえば...**

いくつかの文章を与える。するとそのあとのパラグラフを想定して文章を AI がしたためる。その完成度は与えた文章を書いた人が書いたかのような文章であった。

https://maraoz.com/2020/07/18/openai-gpt3/

たとえば...

各州の人口は?とたずねたらエクセルで表をまとめ始めてくれます。

https://twitter.com/pavtalk/status/1285410751092416513?s=20

**たとえば...**

TODO アプリの作成を依頼する... 3 秒で REACT 製のアプリを作成しました...

https://twitter.com/sharifshameem/status/1284421499915403264?s=20

はあ？何をいっているんだ？

www

**プログラマーいらんやんｗ**

事務職もいらんやんｗ

これらの結果を出すのに長大なトレーニングデータはいりません。すでにマイクロソフトのスーパーコンピュータで学習した状態の AI なのです。それも映画にでてくるようなストーリーでマイクロソフトのクラウドのスーパーコンピュータを用いて膨大なネット能美に在る文献からすでに世界のことを学習済みなのです。

つまりもうレディー状態の怪物（？）

## GPT-3 とは

GPT シリーズは 2015 年にサンフランシスコに設立された AI 系の会社である openAI が開発した自然言語系の AI システムである。openAI はイーロン・マスクも創業者として名を連ねておりそのポテンシャルが感じられるであろう。

Generative Pretrained Transformer 3 の略で、事前にインターネットにある文章などでしっかりトレーニング済みの教師なしの言語が得意な AI です。３は３個めの作品ということです。

GPT-2 の時点で完成度が高すぎて危険ではないかと言われており技術的な詳細はふせられていました。今回は GPT-2 の１００倍以上ものパラメータをインプットし鍛えられた状態のものが公開されたわけです。

GPT-3 に多少の言語を与えるとその次のパラグラフを予見して作成をします。すでに述べたように世界にある膨大な文章でプログラムコードも含んで学習ずみなので、使用者はちょっと情報を入れてあげるだけで、かれは我々を超えた回答をもたらします。

## 英語だけなのか？

かれはあらゆる言語をまなんでいるようです。下記は Github の学習に使われたドキュメントの割合を示すデータです。10 位に ja とあります。日本語もわずかながら学んだようです。英語のデータが 93％なので英語が一番得意でしょうね。

![スクリーンショット 2020-07-21 23.50.13](/media/スクリーンショット 2020-07-21 23.50.13.png)

## 人類はやばいのか？

少し安心したのですが、いろいろな解説を読む限り彼は彼自身のやっている事自体の理解が無いようです。抽象化という概念を身に着けていないため、膨大な学習からのアウトプットをしているだけのようです。

まあ抽象概念を身に着けて創造をされてしまっては、それ自体がシンギュラリティですからね。多少安心しています。

##　参考資料

https://en.wikipedia.org/wiki/OpenAI

https://webbigdata.jp/ai/post-6070

https://github.com/openai/gpt-3

https://openai.com/

https://www.axion.zone/openai-releases-gpt3/

http://deeplearning.hatenablog.com/entry/gpt3

https://arxiv.org/abs/2005.14165
